Project 2

Approach

This program works as a web crawler on Fakebook, finding out the 5 secret flags hidden there. It can be divided into  2
parts: login and crawl.

Login is required for crawling the flags using the given credentials. Otherwise, crawling would fail.
In this program, we assume that the log-in form for Fakebook is available at
http://cs5700fa20.ccs.neu.edu/accounts/login/?next=/fakebook/. Set up a connect to this url via a socket, and send a POST
request along with the form data, including credential and csrftoken, to successfully log in. In the response header,
we can get the sessionid, which is necessary for the following crawling. Start crawling from /fakebook/.

To do crawling, send GET request along with the csrftoken and sessionid mentioned before as the cookie, getting the
content of the web page and using regex to find the formatted flags. The crawler should go through all the links available
on the page. But web pages may include links that point to arbitrary domains. Crawler should only traverse URLs that
point to pages on cs5700fa20.ccs.neu.edu by limit the links using regex /{path}/, which must be paths in the target domain.

To track the frontier, a queue is a structure we used to keep the urls to be crawled. And we also use a set to track all
the valid paths. When it comes to another link, determine if it's in the set already. If so, it means it has been crawled;
Otherwise, add it to the queue and get it ready to be crawled later. In this way, the loops can be avoided.

In addition to crawling Fakebook, our web crawler should be able to correctly handle different HTTP status codes. Function
statusHandler() will use regex to find the status code after send GET request and get the content of the web page. According
to different status codes, our web crawler will do different actions:
1. 500 - Internal Server Error: our web crawler will the request for the URL by using while loop until the request is 
successful. 
2. 301 - Moved Permanently: our web crawler will use regex to get new URL given by the server and try the request again 
with new URL
3. 403 - Forbidden and 404 - Not Found: our crawler will  abandon the URL that generated the error code.
4. 200 OK: our crawler will go through all the links available on the page and add them to frontier.

Challenge

- In the program, a synchronized queue is used to track the frontier. In Python3, it's imported from queue. But the
Python2.7 installed on the Linux machine, it should be imported from Queue, and the constructor is Queue.Queue()

- Cookie Management: Fakebook uses cookies to track whether clients are logged in to the site. If the crawler
successfully logs in to Fakebook using an HTTP POST, Fakebook will return a sessionid cookie. The crawler stores this
cookie, and submit it along with each HTTP GET request as it crawls Fakebook. If the crawler fails to handle cookies
properly, then the software will not be able to successfully crawl Fakebook.

- For POST, The Content-Length header is mandatory for messages with entity bodies, unless the message is transported
using chunked encoding. Content-Length is needed to detect premature message truncation when servers crash and to
properly segment messages that share a persistent connection.

Test

To test the connect, with the help of try catch block, we can simply detect if there is failure. When the connection is
set up, print message send/received to keep track of the communication.
Test for the number and format of arguments in the executable file is also necessary.

Rouni Yin implements the login process and cookie management in the program, setting up the set and queue for the crawler,
and generation for the requests used in the crawler.

Chihao Sun implements the status code handler to check status code and Take different actions according to different
status codes. And implements the code that make our web crawler execute on the command line using the following syntax:
./webcrawler [username] [password]




